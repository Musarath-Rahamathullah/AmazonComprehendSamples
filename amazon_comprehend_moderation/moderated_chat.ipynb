{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a88b9bd-d587-4760-94dc-ea7219a5ca61",
   "metadata": {},
   "source": [
    "# Moderated Chat application with `AmazonComprehendModerationChain`\n",
    "---\n",
    "\n",
    "Install the necessary libraries. We use [Gradio](https://www.gradio.app/) for the Chat UI interface.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You must use a Python3 kernel to run this notebook.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d098bf-b48b-4e5f-9d73-44eb5ec276cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U gradio boto3 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34050a-147e-43c7-8bfd-d329f219d82c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain_experimental huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f5e2a-fb42-45f7-b49a-bff43c954415",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc5061-cf07-4cef-91f6-008436011ce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sample Chat application with HuggingFace Hub \n",
    "---\n",
    "\n",
    "You can use the sample prompt below to start testing the chatbot:\n",
    "\n",
    "```\n",
    "What is John Doe's address, phone number and SSN from the following text?\n",
    "\n",
    "John Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 123-45-6789, reminding him to store it in a safer place.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0babe13-3316-49c5-af0b-7f9e4b982efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "comprehend_client = boto3.client('comprehend', region_name='us-east-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadcc7d-a9bf-4831-858e-4a33b21a2154",
   "metadata": {},
   "source": [
    "Get your API Key from Huggingface hub - https://huggingface.co/docs/api-inference/quicktour#get-your-api-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdde42-30a0-4ed6-a361-da579b19617f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fae5d8-eb06-47aa-b18d-7c1621a0e059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683cea3-61af-4672-8ca9-d630aadd1bcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You must have access to Amazon Comprehend Toxicity and Intent APIs to be able to use the code below as is. Intent and Toxicity APIs are currently under limited preview. Please see the blog for more details on how to get early access to these APIs. If you do not have access to these APIs, you can still use this chat demo with PII check only by using a configuration with PII filter only. Refer to the \"<b>Using moderation_config to customize your moderation</b>\" section in the previous notebook for more details.\n",
    "</div>\n",
    "\n",
    "We build a Gradio chat application demo using HuggingFace Hub model. We use `AmazonComprehendModerationChain` with default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b64d5-4743-4d00-8850-89e84b88e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_experimental.comprehend_moderation import (AmazonComprehendModerationChain,                                                          \n",
    "                                                         BaseModerationConfig, \n",
    "                                                         ModerationIntentConfig, \n",
    "                                                         ModerationPiiConfig, \n",
    "                                                         ModerationToxicityConfig)\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "def respond(message, chat_history):    \n",
    "    repo_id = \"google/flan-t5-xxl\" # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "    template = \"\"\"{question}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
    "    )\n",
    "    \n",
    "    mod_config = BaseModerationConfig(filters=[ModerationPiiConfig()])\n",
    "    comprehend_moderation = AmazonComprehendModerationChain(client=comprehend_client, \n",
    "                                                            moderation_config = mod_config,\n",
    "                                                            verbose=False)\n",
    "\n",
    "    try:\n",
    "        chain = (prompt \n",
    "                 | comprehend_moderation \n",
    "                 | {\"input\": (lambda x: x['output'] ) | llm}\n",
    "                 | comprehend_moderation \n",
    "                )\n",
    "        print(f\"Message= {message}\")\n",
    "        response = chain.invoke({\"question\": message})\n",
    "        return response['output']\n",
    "    except Exception as e:\n",
    "        error_msg = f\"<b style='color:red;'>Error: {str(e)}</b>\"\n",
    "        return error_msg\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.ChatInterface(respond)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb731a-9ea7-41e6-91d3-16c0e98447da",
   "metadata": {},
   "source": [
    "## Chatbot with `AmazonComprehendModerationChain` with Configuration\n",
    "---\n",
    "\n",
    "In the previous demo we looked at a sample to do moderation without passing any moderation config. In this example we will explicityly pass only PII moderation config. But, feel free to use the control panel on the right side of the chat to enable or disable a particular filter.\n",
    "\n",
    "Note:\n",
    " - You must have access to the Toxicity and Intent APIs which are currently under limited preview\n",
    " - If you de-select all the filters then by default all filters will apply\n",
    "\n",
    "You can use the sample prompt below to start testing the chatbot:\n",
    "\n",
    "```\n",
    "What is John Doe's address, phone number and SSN from the following text?\n",
    "\n",
    "John Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 123-45-6789, reminding him to store it in a safer place.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f9a2f-44bc-4a37-83ed-16ecc466e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_experimental.comprehend_moderation import (AmazonComprehendModerationChain,                                                          \n",
    "                                                         BaseModerationConfig, \n",
    "                                                         ModerationIntentConfig, \n",
    "                                                         ModerationPiiConfig, \n",
    "                                                         ModerationToxicityConfig)\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "config_filter = {\"PII\": ModerationPiiConfig(redact=True, mask_character=\"X\"), \"Toxicity\": None, \"Intent\": None}\n",
    "\n",
    "def respond(message, chat_history):    \n",
    "    repo_id = \"google/flan-t5-xxl\" # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "    template = \"\"\"Question: {question}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 256}\n",
    "    )\n",
    "    \n",
    "    filters = [value for value in config_filter.values() if value is not None]\n",
    "\n",
    "    mod_config = BaseModerationConfig(filters=filters) if filters else BaseModerationConfig()\n",
    "    comprehend_moderation = AmazonComprehendModerationChain(client=comprehend_client, \n",
    "                                                            moderation_config = mod_config,\n",
    "                                                            verbose=False)\n",
    "\n",
    "    try:\n",
    "        chain = (prompt \n",
    "                 | comprehend_moderation \n",
    "                 | {\"input\": (lambda x: x['output'] ) | llm}\n",
    "                 | comprehend_moderation \n",
    "                )\n",
    "        response = chain.invoke({\"question\": message})\n",
    "        return response['output']\n",
    "    except Exception as e:\n",
    "        error_msg = f\"<b style='color:red;'>Error: {str(e)}</b>\"\n",
    "        return error_msg\n",
    "\n",
    "def on_select(evt: gr.SelectData):\n",
    "    if evt.value == \"PII\":\n",
    "        config_filter[evt.value] = None if not evt.selected else ModerationPiiConfig(threshold=0.5, mask_character=\"X\", redact=True)\n",
    "    elif evt.value == \"Toxicity\":\n",
    "        config_filter[evt.value] = None if not evt.selected else ModerationToxicityConfig(threshold=0.6)\n",
    "    else:\n",
    "        config_filter[evt.value] = None if not evt.selected else ModerationIntentConfig(threshold=0.8)\n",
    "    return f\"You selected {evt.value} at {evt.index} from {evt.target} and selection is {evt.selected}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.ChatInterface(respond)\n",
    "        with gr.Column(scale=0):\n",
    "            pii = gr.Checkbox(label=\"PII\", value=True, interactive=True) \n",
    "            pii.select(on_select)\n",
    "            tox = gr.Checkbox(label=\"Toxicity\", value=False, interactive=True)                \n",
    "            tox.select(on_select)\n",
    "            intent = gr.Checkbox(label=\"Intent\", value=False, interactive=True)                \n",
    "            intent.select(on_select)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a90321-bc1b-4576-8bc4-6c0a0412d881",
   "metadata": {},
   "source": [
    "#### Troubleshooting\n",
    "---\n",
    "\n",
    "If you receive the following error\n",
    "\n",
    "```\n",
    "'Comprehend' object has no attribute 'detect_toxic_content'\n",
    "```\n",
    "\n",
    "this means that you likely do not have access to the Toxicity API, or you may be in the incorrect region.\n",
    "\n",
    "If you receive the following error\n",
    "\n",
    "```\n",
    "An error occurred (NotAuthorizedException) when calling the ClassifyDocument operation: Your account is not authorized to use Prompt Classification feature.\n",
    "```\n",
    "\n",
    "This means that you do not have access to the intent classification API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675f886-4159-495d-a4d0-a282876f4fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
