{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43eeae79",
   "metadata": {},
   "source": [
    "# Getting insight from customer reviews using Amazon Comprehend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958be25",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id=\"Introduction\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d0e7d",
   "metadata": {},
   "source": [
    "We will use a NLP AI Service from Amazon Web Services - [Amazon Comprehend](https://aws.amazon.com/comprehend/) to solve the business problem. Amazon Comprehend is a natural language processing (NLP) service that uses machine learning (ML) to find insights and relationships in texts. Amazon Comprehend has ability for you to train it to recognize custom entities and perform custom classification. \n",
    "\n",
    "*Notes*: `boto3`, the Python SDK for AWS, is used in the different examples of this notebook. It is already installed if you are executing this Notebook from a Sagemaker Notebook environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e308c9",
   "metadata": {},
   "source": [
    "## Problem Statetment\n",
    "<a id=\"ProblemStatetement\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbea3d",
   "metadata": {},
   "source": [
    "Consumers are increasingly engaging with businesses through digital surfaces and multiple touch-points. Statistics shows that the majority of shoppers use reviews to determine what products to buy and which services to purchase. Reviews have the power to influence consumer decisions and strengthen brand value. Customer review is a great tool to estimate product quality, identify improvement opportunities, launch promotional campaigns and make great product recommendations. We will use Amazon Comprehend to extract meaningful information from product reviews, analyze it to understand how users of different demographies are reacting to products, and also analyze aggregated information on user affinity towards a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9154a",
   "metadata": {},
   "source": [
    "## Use AWS NLP Service Amazon Comprehend as a Solution\n",
    "<a id=\"Rescue\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f4396",
   "metadata": {},
   "source": [
    "We will use Natural Language Processing to solve the problem by following the below mentioned approach - \n",
    "\n",
    "#### 1. Data Processing and Transformation Notebook\n",
    "Exploratory Data Analysis to understand the dataset\n",
    "#### 2. Comprehend Topic Modelling Job Notebook\n",
    "Use Topic Modeling to generate topics\n",
    "#### 3. Topic Mapping and Sentiment Generation Notebook\n",
    "Use topics to understand segments and sentiment associated with each item\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b567c",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7d747",
   "metadata": {},
   "source": [
    "#### Initialize Input & Output Paths\n",
    "<a id=\"InitialiazeS3Data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e416495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16071eb6",
   "metadata": {},
   "source": [
    "### Input-paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket containing the data\n",
    "BUCKET = 'clothing-shoe-jewel-tm-blog'\n",
    "\n",
    "# Item ratings and metadata\n",
    "S3_DATA_FILE = 'Clothing_Shoes_and_Jewelry.json.gz' # Zip\n",
    "S3_META_FILE = 'meta_Clothing_Shoes_and_Jewelry.json.gz' # Zip\n",
    "\n",
    "S3_DATA = 's3://' + BUCKET + '/' + S3_DATA_FILE\n",
    "S3_META = 's3://' + BUCKET + '/' + S3_META_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481086d",
   "metadata": {},
   "source": [
    "### Output-paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f62365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformed review, input for Comprehend\n",
    "LOCAL_TRANSFORMED_REVIEW = os.path.join('data', 'TransformedReviews.txt')\n",
    "S3_OUT = 's3://' + BUCKET + '/out/' + 'TransformedReviews.txt'\n",
    "\n",
    "# Final dataframe where topics and sentiments are going to be joined\n",
    "S3_FEEDBACK_TOPICS = 's3://' + BUCKET + '/out/' + 'FinalDataframe.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93528f",
   "metadata": {},
   "source": [
    "#### Load Review and Meta Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_df(path):\n",
    "    \"\"\"Reads a subset of a json file in a given path in chunks, combines, and returns\n",
    "    \"\"\"\n",
    "    # Creating chunks from 500k data points each of chunk size 10k\n",
    "    chunks = pd.read_json(path, orient='records', \n",
    "                                lines=True, \n",
    "                                nrows=500000, \n",
    "                                chunksize=10000, \n",
    "                                compression='gzip')\n",
    "    # Creating a single dataframe from all the chunks\n",
    "    load_df = pd.DataFrame()\n",
    "    for chunk in chunks:\n",
    "        load_df = pd.concat([load_df, chunk], axis=0)\n",
    "    return load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f568612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review data\n",
    "original_df = convert_json_to_df(S3_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "original_meta = convert_json_to_df(S3_META)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5247f",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of reviews and metadata\n",
    "print('Shape of review data: ', original_df.shape)\n",
    "print('Shape of metadata: ', original_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in verified reviews only\n",
    "# Also checking the amount of missing values in the review data\n",
    "print('Frequency of verified/non verified review data: ', original_df['verified'].value_counts())\n",
    "print('Frequency of missing values in review data: ', original_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sneak peek for review data\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42afb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sneak peek for metadata\n",
    "original_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9159836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each categories for EDA.\n",
    "print('Frequncy of different item categories in metadata: ', original_meta['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b57789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values for metadata\n",
    "print('Frequency of missing values in metadata: ', original_meta.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are duplicated data. There are indeed duplicated data in the dataframe.\n",
    "print('Duplicate items in metadata: ', original_meta[original_meta['asin'].duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801a8ce",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    \"\"\"Preprocessing review text.\n",
    "    The text becomes Comprehend compatible as a result.\n",
    "    This is the most important preprocessing step.\n",
    "    \"\"\"\n",
    "    # Encode and decode reviews\n",
    "    df['reviewText'] = df['reviewText'].str.encode(\"utf-8\", \"ignore\")\n",
    "    df['reviewText'] = df['reviewText'].str.decode('ascii')\n",
    "\n",
    "    # Replacing characters with whitespace\n",
    "    df['reviewText'] = df['reviewText'].replace(r'\\r+|\\n+|\\t+|\\u2028',' ', regex=True)\n",
    "\n",
    "    # Replacing punctuations\n",
    "    df['reviewText'] = df['reviewText'].str.replace('[^\\w\\s]','', regex=True)\n",
    "\n",
    "    # Lowercasing reviews\n",
    "    df['reviewText'] = df['reviewText'].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(df):\n",
    "    \"\"\"Encoding and getting reviews in byte size.\n",
    "    Review gets encoded to utf-8 format and getting the size of the reviews in bytes. \n",
    "    Comprehend requires each review input to be no more than 5000 Bytes\n",
    "    \"\"\"\n",
    "    df['review_size'] = df['reviewText'].apply(lambda x:len(x.encode('utf-8')))\n",
    "    df = df[(df['review_size'] > 0) & (df['review_size'] < 5000)]\n",
    "    df = df.drop(columns=['review_size'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only data points with a verified review will be selected and the review must not be missing\n",
    "filter = (original_df['verified'] == True) & (~original_df['reviewText'].isna())\n",
    "filtered_df = original_df[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only a subset of fields are selected in this experiment. \n",
    "filtered_df = filtered_df[['asin', 'reviewText', 'summary', 'unixReviewTime', 'overall', 'reviewerID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaea908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, once again, dropping data points with missing review text\n",
    "filtered_df = filtered_df.dropna(subset=['reviewText'])\n",
    "print('Shape of review data: ', filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cec4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate items from metadata\n",
    "original_meta = original_meta.drop_duplicates(subset=['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5629207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only a subset of fields are selected in this experiment. \n",
    "original_meta = original_meta[['asin', 'category', 'title', 'description', 'brand', 'main_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean reviews using text cleaning pipeline\n",
    "df = clean_text(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb104e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index as we are merging metadata with reviews shortly\n",
    "df = df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7efa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata with review data\n",
    "df = df.merge(original_meta, how='left', on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe where Comprehend outputs (topics and sentiments) will be added\n",
    "df = prepare_input_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187c57d",
   "metadata": {},
   "source": [
    "### Save Data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150babe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframe on S3\n",
    "df.to_csv(S3_FEEDBACK_TOPICS, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b06d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews are transformed per Comprehend guideline- one review per line\n",
    "# The txt file will be used as input for Comprehend\n",
    "# We first save the input file locally\n",
    "with open(LOCAL_TRANSFORMED_REVIEW, \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(df['reviewText'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferring the transformed review (input to Comprehend) to S3\n",
    "!aws s3 mv {LOCAL_TRANSFORMED_REVIEW} {S3_OUT}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
